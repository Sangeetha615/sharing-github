{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06792992745623491\n",
      "[[ 1.27472406  1.40730486  2.50479382 -1.75940996 -2.82299676 -2.40510068]\n",
      " [ 1.5730311   3.9381807  -0.13045194  1.19883806 -2.40549672  1.57571661]\n",
      " [ 2.78489456 -0.28614894 -3.59008268  2.31382869  3.74737482 -0.76078668]\n",
      " [-2.49200891 -2.0602105  -0.72792808  1.45598936  1.69251133 -0.00498203]\n",
      " [ 2.55048319  1.65645895 -3.58797945 -3.49316599 -0.36960214 -0.36210787]\n",
      " [ 0.31727294  1.76110359 -0.34144089  2.99390757  3.2674886  -0.36543652]\n",
      " [-2.19705978 -1.57342883 -0.84578744  3.74948896  3.08082378  0.64181778]\n",
      " [ 3.84327441  1.70902703 -0.56486885  1.99247607  2.4070048  -2.414635  ]\n",
      " [-3.30740751 -3.43982509  3.72474825  2.86420101 -2.4106809   3.71231301]\n",
      " [ 2.22055712  0.51065178  3.14426353 -2.23608415 -0.39063604  3.50610813]\n",
      " [-1.01599283 -3.41725283 -0.44951805  0.26064201  0.8332503  -0.77856214]\n",
      " [ 3.76154802 -2.70301631  3.34525477 -1.86292097 -3.07859787  0.17429343]\n",
      " [ 1.11915723 -3.21497171  0.17746909  1.96975205  2.83576072 -3.24887364]\n",
      " [ 0.98296342 -2.58141471  1.09467728 -2.73583538 -2.74221454  0.06430154]\n",
      " [ 1.01030441 -2.97763299 -2.36883971 -3.59101482 -3.46550834 -0.42920505]\n",
      " [ 0.90386864 -1.27000542 -0.60209692  3.92974811  2.86692777 -3.18674711]\n",
      " [-3.6090017   1.72291211  2.66766785  2.24304252 -0.32718546  1.11426581]\n",
      " [ 0.37062621  1.89985563 -0.73812474  2.88145231  3.98726816 -2.33037468]\n",
      " [-3.75730789 -2.11247023  1.96404313  0.7718264   2.97330984 -1.45693373]\n",
      " [ 0.90309199  2.97162731 -2.96124479 -0.91930226 -3.64681636  2.12597555]\n",
      " [-3.69369833 -1.11177949  3.28136654 -2.0357871  -3.54124093  0.45867309]\n",
      " [-1.26662605  2.71910081  2.42584018  1.33021793 -3.57688897  2.01101534]\n",
      " [-0.20942958 -2.47320833 -0.59954498 -0.79576465 -1.62396265 -0.13416687]\n",
      " [-2.32790347  0.05208385 -1.47452928  1.74974162 -2.71233346 -0.91273232]\n",
      " [-2.24933393 -1.79989107  0.91771454 -1.44643941 -0.07680925  1.38589819]\n",
      " [ 3.92500461  0.29148381 -2.17716492 -0.91476416  2.90032899 -2.99214724]\n",
      " [ 2.27539305 -1.67258967 -2.35751378  1.48921119 -2.9144989  -1.59215541]\n",
      " [ 3.18209777  1.55579464  1.31917542  3.39624444 -3.67663194  1.6548954 ]\n",
      " [-0.01505444 -3.91249791  2.02209984  1.41343021 -1.95690496  0.02666931]\n",
      " [-1.6787514  -2.63942513 -3.17709026 -0.94525432 -3.87702602 -1.3312225 ]]\n",
      "Generation :  0\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334 [ 3.18211217e-02  4.57928065e-02  4.93780406e-02  5.15504419e-02\n",
      "  5.19314076e-02  5.33236696e-02  5.53288086e-02  5.99383464e-02\n",
      "  6.08135141e-02  6.11087624e-02  6.79299275e-02  7.25050807e-02\n",
      "  7.97098379e-02  8.45061990e-02  9.97876407e-02  1.06344558e-01\n",
      "  1.08818541e-01  1.13299366e-01  1.27556707e-01  1.30927372e-01\n",
      "  1.31974920e-01  1.58529695e-01  1.67985123e-01 -1.00000000e+11\n",
      " -1.00000000e+11]\n",
      "Generation :  1\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334 [ 3.18211217e-02  4.57928065e-02  4.93780406e-02  5.15504419e-02\n",
      "  5.19314076e-02  5.33236696e-02  5.53288086e-02  5.99383464e-02\n",
      "  6.08135141e-02  6.11087624e-02  6.79299275e-02  7.25050807e-02\n",
      "  7.97098379e-02  8.45061990e-02  9.97876407e-02  1.06344558e-01\n",
      "  1.08818541e-01  1.13299366e-01  1.27556707e-01  1.30927372e-01\n",
      "  1.31974920e-01  1.58529695e-01  1.67985123e-01 -1.00000000e+11\n",
      " -1.00000000e+11]\n",
      "Generation :  2\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334 [ 3.18211217e-02  4.57928065e-02  4.93780406e-02  5.15504419e-02\n",
      "  5.19314076e-02  5.33236696e-02  5.53288086e-02  5.99383464e-02\n",
      "  6.08135141e-02  6.11087624e-02  6.79299275e-02  7.25050807e-02\n",
      "  7.97098379e-02  8.45061990e-02  9.97876407e-02  1.06344558e-01\n",
      "  1.08818541e-01  1.13299366e-01  1.27556707e-01  1.30927372e-01\n",
      "  1.31974920e-01  1.58529695e-01  1.67985123e-01 -1.00000000e+11\n",
      " -1.00000000e+11]\n",
      "Generation :  3\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334 [ 3.18211217e-02  4.57928065e-02  4.93780406e-02  5.15504419e-02\n",
      "  5.19314076e-02  5.33236696e-02  5.53288086e-02  5.99383464e-02\n",
      "  6.08135141e-02  6.11087624e-02  6.79299275e-02  7.25050807e-02\n",
      "  7.97098379e-02  8.45061990e-02  9.97876407e-02  1.06344558e-01\n",
      "  1.08818541e-01  1.13299366e-01  1.27556707e-01  1.30927372e-01\n",
      "  1.31974920e-01  1.58529695e-01  1.67985123e-01 -1.00000000e+11\n",
      " -1.00000000e+11]\n",
      "Generation :  4\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334 [ 3.18211217e-02  4.57928065e-02  4.93780406e-02  5.15504419e-02\n",
      "  5.19314076e-02  5.33236696e-02  5.53288086e-02  5.99383464e-02\n",
      "  6.08135141e-02  6.11087624e-02  6.79299275e-02  7.25050807e-02\n",
      "  7.97098379e-02  8.45061990e-02  9.97876407e-02  1.06344558e-01\n",
      "  1.08818541e-01  1.13299366e-01  1.27556707e-01  1.30927372e-01\n",
      "  1.31974920e-01  1.58529695e-01  1.67985123e-01 -1.00000000e+11\n",
      " -1.00000000e+11]\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best solution :  [[[-2.24933393 -1.79989107  0.91771454  1.74974162 -2.21326328\n",
      "   -0.91273232]]]\n",
      "Best solution fitness :  [0.27232927]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy\n",
    "from numpy import delete\n",
    "import math\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "def read_article(para):\n",
    "    file = open( r\"C:\\Users\\user\\vijay.txt\",encoding='utf-8')\n",
    "    filedata = file.read()\n",
    "#     print(filedata)\n",
    "    sentences = sent_tokenize(filedata)\n",
    "    #print(\"Tokenized Sentences: \",sentences)\n",
    "    sent=word_tokenize(filedata)\n",
    "#     print(sentences)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    #ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        #print(words)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            #word = ps.stem(word)\n",
    "            #print(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "         \n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "            \n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    sum=0\n",
    "    text=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "            \n",
    "        \n",
    "            #print(tf_idf_table[word1])\n",
    "        tf_idf_matrix[sent1]= tf_idf_table\n",
    "#         print(tf_idf_matrix[sent1])\n",
    "        \n",
    "    return tf_idf_matrix\n",
    "def _tf_idf(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    sum=0\n",
    "    text=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "            \n",
    "        \n",
    "            #print(tf_idf_table[word1])\n",
    "            tf_idf_matrix[sent1]= tf_idf_table\n",
    "#         print(tf_idf_matrix[sent1])\n",
    "        s=len(tf_idf_matrix[sent1])\n",
    "#         print(s)\n",
    "        for i in tf_idf_matrix[sent1].values():\n",
    "            \n",
    "            sum=(sum+i)\n",
    "            \n",
    "        text.append(sum/s)\n",
    "        sum=0 \n",
    "    i=23  \n",
    "    print(text[i])\n",
    "    return text\n",
    "def cosine_value(sentences):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(sentences)\n",
    "\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=[sentences])\n",
    "    df\n",
    "    cosine=cosine_similarity(df, df)\n",
    "#     print(cosine)\n",
    "    return cosine\n",
    "def cal_pop_fitness(text, pop):\n",
    "    n=[]\n",
    "    arr=[]\n",
    "    arr1=[]\n",
    "    final=[]\n",
    "    final1=[]\n",
    "    score=[]\n",
    "    file = open( r\"C:\\Users\\user\\vijay.txt\",encoding='utf-8')\n",
    "    filedata = file.read()\n",
    "#     print(filedata)\n",
    "    sentences = sent_tokenize(filedata)\n",
    "    cosine=cosine_value(sentences)\n",
    "    x=np.array(cosine)\n",
    "    y=np.argwhere(x>0.8)\n",
    "#     z=np.unique(y)\n",
    "\n",
    "    k=0\n",
    "    for i in range(len(y)): \n",
    "        if(y[i][k] ==y[i][k+1]): \n",
    "            continue\n",
    "\n",
    "        elif(text[y[i][k]]>text[y[i][k+1]]):\n",
    "            n.append(text[y[i][k]])\n",
    "            arr.append(y[i][k])\n",
    "            arr.append(y[i][k+1])\n",
    "          \n",
    "        else:\n",
    "            n.append(text[y[i][k+1]])\n",
    "#     print(np.unique(n))#returning the greater score\n",
    "    arr1=np.unique(arr)\n",
    "#     print(arr1)\n",
    "#     print(y)\n",
    "#     print(arr1)#returning the comparing sentences\n",
    "    for i in range(len(y)):\n",
    "        if (y[i][k] not in arr1):\n",
    "            final.append(y[i][k])\n",
    "#     print(final)#returning the other sentences\n",
    "    final1=np.unique(final)\n",
    "#     print(final1)\n",
    "#     print(len(final1))\n",
    "    for i in range(len(final1)-1):\n",
    "        score.append(text[final1[i]])\n",
    "#     print(score)# returning the other sentence score\n",
    "    final_list=[n+score]\n",
    "    final_score=np.unique(final_list)\n",
    "    print(\"final_score  \",final_score)\n",
    "    return final_score\n",
    "def select_mating_pool(pop, fitness, num_parents):\n",
    "    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n",
    "    parents = numpy.empty((num_parents, pop.shape[1]))\n",
    "    for parent_num in range(num_parents):\n",
    "        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "        max_fitness_idx = max_fitness_idx[0][0]\n",
    "        parents[parent_num, :] = pop[max_fitness_idx, :]\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "    return parents\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = numpy.empty(offspring_size)\n",
    "    # The point at which crossover takes place between two parents. Usually it is at the center.\n",
    "    crossover_point = numpy.uint8(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        # The new offspring will have its first half of its genes taken from the first parent.\n",
    "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "        # The new offspring will have its second half of its genes taken from the second parent.\n",
    "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutation(offspring_crossover):\n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        # The random value to be added to the gene.\n",
    "        random_value = numpy.random.uniform(-1.0, 1.0, 1)\n",
    "        offspring_crossover[idx, 4] = offspring_crossover[idx, 4] + random_value\n",
    "    return offspring_crossover\n",
    "\n",
    "\n",
    "def generate_summary(para, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(para)\n",
    "    \n",
    "    total_documents = len(sentences)\n",
    "    \n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(\"freqency_matrix\",freq_matrix)\n",
    "\n",
    "    \n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(\"tf_matrix:\",tf_matrix)\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    \n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(\"idf_matrix\",idf_matrix)\n",
    "\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(\"\\n\\n\")\n",
    "    #print(\"tf-idf: \",tf_idf_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "# Create the Document Term Matrix\n",
    "    \n",
    "#genetic optimizer part begin\n",
    "    text=_tf_idf(tf_matrix, idf_matrix)\n",
    "    num_weights=6\n",
    "    sol_per_pop = 30\n",
    "    num_parents_mating = 2\n",
    "    # Defining the population size.\n",
    "    pop_size = (sol_per_pop,num_weights) # The population will have sol_per_pop chromosome where each chromosome has num_weights genes.\n",
    "#Creating the initial population.\n",
    "    new_population = numpy.random.uniform(low=-4.0, high=4.0, size=pop_size)\n",
    "    print(new_population)\n",
    "    num_generations = 5\n",
    "    for generation in range(num_generations):\n",
    "        print(\"Generation : \", generation)\n",
    "    # Measing the fitness of each chromosome in the population.\n",
    "        fitness = cal_pop_fitness(text, new_population)\n",
    "        # Selecting the best parents in the population for mating.\n",
    "        parents = select_mating_pool(new_population, fitness, num_parents_mating)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "        offspring_crossover = crossover(parents,\n",
    "                                       offspring_size=(pop_size[0]-parents.shape[0], num_weights))\n",
    "\n",
    "    # Adding some variations to the offsrping using mutation.\n",
    "        offspring_mutation = mutation(offspring_crossover)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "        new_population[0:parents.shape[0], :] = parents\n",
    "        new_population[parents.shape[0]:, :] = offspring_mutation\n",
    "\n",
    "    # The best result in the current iteration.\n",
    "        print(\"Best result : \", numpy.max(fitness),fitness)#numpy.sum(new_population*text, axis=1)))\n",
    "\n",
    "# Getting the best solution after iterating finishing all generations.\n",
    "#At first, the fitness is calculated for each solution in the final generation.\n",
    "#     text=_tf_idf(tf_matrix, idf_matrix)\n",
    "    fitness = cal_pop_fitness(text, new_population)\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "    best_match_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "\n",
    "    print(\"Best solution : \", new_population[best_match_idx, :])\n",
    "    print(\"Best solution fitness : \", fitness[best_match_idx])\n",
    "\n",
    "\n",
    "generate_summary( \"msft.txt\", 2)\n",
    "#Genetic algorithm part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
