{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06792992745623491\n",
      "[[ 6.24303900e-01 -2.34162549e+00 -1.65453658e+00  3.77760547e+00\n",
      "   3.14392455e+00  7.84839918e-02]\n",
      " [-2.75771015e+00 -3.15270143e+00  1.12691524e+00 -3.33850444e+00\n",
      "  -2.27456189e+00 -4.53237743e-01]\n",
      " [ 2.62105084e+00 -1.30483092e+00  1.75191055e+00  3.22065227e-02\n",
      "   2.04069530e+00  2.75438578e+00]\n",
      " [-3.64771425e+00 -1.78413460e+00  3.12386019e+00  3.96212567e+00\n",
      "   2.53409635e+00  1.68511959e+00]\n",
      " [ 1.46812712e-01  1.72986956e-01  3.93743344e+00 -1.50242956e+00\n",
      "  -2.77035764e+00 -3.36084382e+00]\n",
      " [ 2.82608864e+00  3.77558898e+00 -1.22010709e+00  3.96490698e+00\n",
      "  -1.38437795e+00  2.01993265e+00]\n",
      " [ 1.52357519e+00 -4.83152205e-01  1.03133173e+00  2.56195717e+00\n",
      "   3.17300550e+00  3.88307084e+00]\n",
      " [ 2.26906961e+00  8.73358530e-01 -1.84389152e+00 -2.73740613e+00\n",
      "  -8.33934012e-01  3.43891190e+00]\n",
      " [ 1.28556718e+00  1.17322768e+00 -2.59054967e+00  8.63743593e-01\n",
      "   3.94403884e+00 -3.52885320e+00]\n",
      " [-1.89899623e+00 -3.89109176e+00  1.01421464e+00 -1.90808399e-01\n",
      "  -6.24965823e-01  5.57506791e-01]\n",
      " [-9.20687292e-01 -7.60980538e-01 -3.66606745e+00  3.37542595e+00\n",
      "  -1.98336378e+00 -9.62338059e-01]\n",
      " [ 3.76844760e+00 -3.74288572e+00  6.49205746e-01  3.76292521e+00\n",
      "  -3.75932791e+00 -1.77804222e+00]\n",
      " [-1.00254547e+00 -1.20475435e+00  1.39788861e+00  3.96831005e-01\n",
      "   2.69426310e+00  2.44864256e+00]\n",
      " [ 1.06133099e-03 -2.53501612e+00 -2.49604504e+00 -2.46467725e+00\n",
      "  -3.14873107e+00 -2.99985481e+00]\n",
      " [ 9.55872343e-01 -5.20284098e-01 -4.41090784e-01  1.36098753e+00\n",
      "   1.17049884e+00 -1.63887548e+00]\n",
      " [ 1.15829991e+00  3.92959119e+00 -3.54693142e+00  3.26420465e-01\n",
      "  -3.66371152e+00  3.57218411e+00]\n",
      " [ 9.34129994e-01 -2.98892585e-01  3.84381655e-01  5.45264868e-01\n",
      "   3.10375498e+00  3.75420080e-01]\n",
      " [ 3.43427944e+00 -2.21434354e+00  2.09189955e+00 -2.89423146e+00\n",
      "  -3.39992000e+00  1.85310444e+00]\n",
      " [ 3.58112803e+00 -3.91364698e+00 -8.02692947e-01 -1.85116633e-01\n",
      "   2.87757971e+00 -1.52392765e-01]\n",
      " [ 1.16526765e+00  1.42005782e+00 -2.85321861e+00 -3.16887004e+00\n",
      "  -3.42373261e+00 -3.95473115e+00]\n",
      " [ 2.65739618e+00 -6.19994233e-01  3.26165480e+00  1.06209933e+00\n",
      "  -3.20151158e+00 -2.28421738e+00]\n",
      " [ 3.46401926e+00 -2.47471185e+00 -1.84083458e+00 -4.66104070e-01\n",
      "  -3.97262925e+00  1.51936338e+00]\n",
      " [ 1.00518985e+00 -2.19803630e-01 -2.97728858e+00 -3.03678876e+00\n",
      "  -3.64604160e+00  1.72481088e+00]\n",
      " [ 2.86904414e+00 -3.01448772e+00  2.66190543e+00  7.63524613e-02\n",
      "   3.95957417e+00 -1.93642514e+00]\n",
      " [ 3.97932403e+00 -2.44689768e-01  3.58778315e+00 -2.15984143e-01\n",
      "   1.36208632e+00  2.99300101e+00]\n",
      " [-6.60270319e-01 -3.49386411e+00 -1.10362791e+00 -3.69784932e+00\n",
      "  -8.32095064e-02  1.68394819e+00]\n",
      " [ 1.32140085e+00 -1.71575337e+00  3.76594649e+00 -3.17090076e+00\n",
      "   1.25262480e+00  3.21234246e+00]\n",
      " [-1.79509982e+00 -2.89065036e+00  3.43188074e+00 -2.47530269e+00\n",
      "   3.86464089e+00  2.61885189e+00]\n",
      " [-1.18139645e+00 -1.12393139e+00  2.49919183e+00 -3.81759764e+00\n",
      "  -1.86465569e+00  1.24290856e-01]\n",
      " [-3.10103003e+00 -2.37379403e+00 -6.73071560e-01 -1.87110492e+00\n",
      "  -3.56660052e+00  1.26888287e+00]]\n",
      "Generation :  0\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334\n",
      "Generation :  1\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334\n",
      "Generation :  2\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334\n",
      "Generation :  3\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334\n",
      "Generation :  4\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best result :  0.1679851233677334\n",
      "final_score   [0.03182112 0.04579281 0.04937804 0.05155044 0.05193141 0.05332367\n",
      " 0.05532881 0.05993835 0.06081351 0.06110876 0.06792993 0.07250508\n",
      " 0.07970984 0.0845062  0.09978764 0.10634456 0.10881854 0.11329937\n",
      " 0.12755671 0.13092737 0.13197492 0.1585297  0.16798512 0.21309221\n",
      " 0.27232927]\n",
      "Best solution :  [[[ 3.97932403 -0.24468977  3.58778315  0.07635246  3.73192632\n",
      "   -1.93642514]]]\n",
      "Best solution fitness :  [0.27232927]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy\n",
    "from numpy import delete\n",
    "import math\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "def read_article(para):\n",
    "    file = open( r\"C:\\Users\\user\\vijay.txt\",encoding='utf-8')\n",
    "    filedata = file.read()\n",
    "#     print(filedata)\n",
    "    sentences = sent_tokenize(filedata)\n",
    "    #print(\"Tokenized Sentences: \",sentences)\n",
    "    sent=word_tokenize(filedata)\n",
    "#     print(sentences)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    #ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        #print(words)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            #word = ps.stem(word)\n",
    "            #print(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "         \n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "            \n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    sum=0\n",
    "    text=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "            \n",
    "        \n",
    "            #print(tf_idf_table[word1])\n",
    "        tf_idf_matrix[sent1]= tf_idf_table\n",
    "#         print(tf_idf_matrix[sent1])\n",
    "        \n",
    "    return tf_idf_matrix\n",
    "def _tf_idf(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    sum=0\n",
    "    text=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "            \n",
    "        \n",
    "            #print(tf_idf_table[word1])\n",
    "            tf_idf_matrix[sent1]= tf_idf_table\n",
    "#         print(tf_idf_matrix[sent1])\n",
    "        s=len(tf_idf_matrix[sent1])\n",
    "#         print(s)\n",
    "        for i in tf_idf_matrix[sent1].values():\n",
    "            \n",
    "            sum=(sum+i)\n",
    "            \n",
    "        text.append(sum/s)\n",
    "        sum=0 \n",
    "    i=23  \n",
    "    print(text[i])\n",
    "    return text\n",
    "def cosine_value(sentences):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(sentences)\n",
    "\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=[sentences])\n",
    "    df\n",
    "    cosine=cosine_similarity(df, df)\n",
    "#     print(cosine)\n",
    "    return cosine\n",
    "def cal_pop_fitness(text, pop):\n",
    "    n=[]\n",
    "    arr=[]\n",
    "    arr1=[]\n",
    "    final=[]\n",
    "    final1=[]\n",
    "    score=[]\n",
    "    file = open( r\"C:\\Users\\user\\vijay.txt\",encoding='utf-8')\n",
    "    filedata = file.read()\n",
    "#     print(filedata)\n",
    "    sentences = sent_tokenize(filedata)\n",
    "    cosine=cosine_value(sentences)\n",
    "    x=np.array(cosine)\n",
    "    y=np.argwhere(x>0.8)\n",
    "#     z=np.unique(y)\n",
    "\n",
    "    k=0\n",
    "    for i in range(len(y)): \n",
    "        if(y[i][k] ==y[i][k+1]): \n",
    "            continue\n",
    "\n",
    "        elif(text[y[i][k]]>text[y[i][k+1]]):\n",
    "            n.append(text[y[i][k]])\n",
    "            arr.append(y[i][k])\n",
    "            arr.append(y[i][k+1])\n",
    "          \n",
    "        else:\n",
    "            n.append(text[y[i][k+1]])\n",
    "#     print(np.unique(n))#returning the greater score\n",
    "    arr1=np.unique(arr)\n",
    "#     print(arr1)\n",
    "#     print(y)\n",
    "#     print(arr1)#returning the comparing sentences\n",
    "    for i in range(len(y)):\n",
    "        if (y[i][k] not in arr1):\n",
    "            final.append(y[i][k])\n",
    "#     print(final)#returning the other sentences\n",
    "    final1=np.unique(final)\n",
    "#     print(final1)\n",
    "#     print(len(final1))\n",
    "    for i in range(len(final1)-1):\n",
    "        score.append(text[final1[i]])\n",
    "#     print(score)# returning the other sentence score\n",
    "    final_list=[n+score]\n",
    "    final_score=np.unique(final_list)\n",
    "    print(\"final_score  \",final_score)\n",
    "    return final_score\n",
    "def select_mating_pool(pop, fitness, num_parents):\n",
    "    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n",
    "    parents = numpy.empty((num_parents, pop.shape[1]))\n",
    "    for parent_num in range(num_parents):\n",
    "        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "        max_fitness_idx = max_fitness_idx[0][0]\n",
    "        parents[parent_num, :] = pop[max_fitness_idx, :]\n",
    "        fitness[max_fitness_idx] = -99999999999\n",
    "    return parents\n",
    "\n",
    "def crossover(parents, offspring_size):\n",
    "    offspring = numpy.empty(offspring_size)\n",
    "    # The point at which crossover takes place between two parents. Usually it is at the center.\n",
    "    crossover_point = numpy.uint8(offspring_size[1]/2)\n",
    "\n",
    "    for k in range(offspring_size[0]):\n",
    "        # Index of the first parent to mate.\n",
    "        parent1_idx = k%parents.shape[0]\n",
    "        # Index of the second parent to mate.\n",
    "        parent2_idx = (k+1)%parents.shape[0]\n",
    "        # The new offspring will have its first half of its genes taken from the first parent.\n",
    "        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n",
    "        # The new offspring will have its second half of its genes taken from the second parent.\n",
    "        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n",
    "    return offspring\n",
    "\n",
    "def mutation(offspring_crossover):\n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    for idx in range(offspring_crossover.shape[0]):\n",
    "        # The random value to be added to the gene.\n",
    "        random_value = numpy.random.uniform(-1.0, 1.0, 1)\n",
    "        offspring_crossover[idx, 4] = offspring_crossover[idx, 4] + random_value\n",
    "    return offspring_crossover\n",
    "\n",
    "\n",
    "def generate_summary(para, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(para)\n",
    "    \n",
    "    total_documents = len(sentences)\n",
    "    \n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(\"freqency_matrix\",freq_matrix)\n",
    "\n",
    "    \n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(\"tf_matrix:\",tf_matrix)\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    \n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(\"idf_matrix\",idf_matrix)\n",
    "\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(\"\\n\\n\")\n",
    "    #print(\"tf-idf: \",tf_idf_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "# Create the Document Term Matrix\n",
    "    \n",
    "#genetic optimizer part begin\n",
    "    text=_tf_idf(tf_matrix, idf_matrix)\n",
    "    num_weights=6\n",
    "    sol_per_pop = 30\n",
    "    num_parents_mating = 2\n",
    "    # Defining the population size.\n",
    "    pop_size = (sol_per_pop,num_weights) # The population will have sol_per_pop chromosome where each chromosome has num_weights genes.\n",
    "#Creating the initial population.\n",
    "    new_population = numpy.random.uniform(low=-4.0, high=4.0, size=pop_size)\n",
    "    print(new_population)\n",
    "    num_generations = 5\n",
    "    for generation in range(num_generations):\n",
    "        print(\"Generation : \", generation)\n",
    "    # Measing the fitness of each chromosome in the population.\n",
    "        fitness = cal_pop_fitness(text, new_population)\n",
    "        # Selecting the best parents in the population for mating.\n",
    "        parents = select_mating_pool(new_population, fitness, num_parents_mating)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "        offspring_crossover = crossover(parents,\n",
    "                                       offspring_size=(pop_size[0]-parents.shape[0], num_weights))\n",
    "\n",
    "    # Adding some variations to the offsrping using mutation.\n",
    "        offspring_mutation = mutation(offspring_crossover)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "        new_population[0:parents.shape[0], :] = parents\n",
    "        new_population[parents.shape[0]:, :] = offspring_mutation\n",
    "\n",
    "    # The best result in the current iteration.\n",
    "        print(\"Best result : \", numpy.max(fitness))#numpy.sum(new_population*text, axis=1)))\n",
    "\n",
    "# Getting the best solution after iterating finishing all generations.\n",
    "#At first, the fitness is calculated for each solution in the final generation.\n",
    "#     text=_tf_idf(tf_matrix, idf_matrix)\n",
    "    fitness = cal_pop_fitness(text, new_population)\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "    best_match_idx = numpy.where(fitness == numpy.max(fitness))\n",
    "\n",
    "    print(\"Best solution : \", new_population[best_match_idx, :])\n",
    "    print(\"Best solution fitness : \", fitness[best_match_idx])\n",
    "\n",
    "\n",
    "generate_summary( \"msft.txt\", 2)\n",
    "#Genetic algorithm part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
